{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af89a3c8-934e-4088-9ff1-72efaa57c176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import json\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224112c6-8905-478f-af93-716ecaec0e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (19.0.0)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.35.87)\n",
      "Collecting smart_open\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (1.35.99)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (2024.12.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.10.4)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from smart_open) (1.17.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart_open\n",
      "Successfully installed smart_open-7.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow s3fs boto3 smart_open\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12ec5fa-8ecb-4485-a19f-1d9998530413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"bigdatateaching\"\n",
    "sub_prefix = \"reddit-project/reddit/parquet/submissions/yyyy=2024/mm=01/\"\n",
    "com_prefix = \"reddit-project/reddit/parquet/comments/yyyy=2024/mm=01/\"\n",
    "\n",
    "submissions = []\n",
    "comments = []\n",
    "\n",
    "sub_response = s3.list_objects_v2(Bucket=bucket_name, Prefix=sub_prefix)\n",
    "com_response = s3.list_objects_v2(Bucket=bucket_name, Prefix=com_prefix)\n",
    "if \"Contents\" in sub_response:\n",
    "    for obj in sub_response[\"Contents\"]:\n",
    "        submissions.append(obj[\"Key\"])  \n",
    "else:\n",
    "    print(\"No files found at this location.\")\n",
    "#print(\"submissions:\", submissions)\n",
    "if \"Contents\" in com_response:\n",
    "    for obj in com_response[\"Contents\"]:\n",
    "        comments.append(obj[\"Key\"])  \n",
    "else:\n",
    "    print(\"No files found at this location.\")\n",
    "#print(\"comments:\", comments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00988438-e0a0-4953-a817-679641437c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=2024/mm=01/submissions_RS_2024-01.zst_1.parquet\n",
      "s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=2024/mm=01/comments_RC_2024-01.zst_1.parquet\n"
     ]
    }
   ],
   "source": [
    "s3_submissions_path = f\"s3://{bucket_name}/{submissions[0]}\"  # First submission file\n",
    "print(s3_submissions_path)\n",
    "s3_comments_path = f\"s3://{bucket_name}/{comments[0]}\"  # First comment file\n",
    "print(s3_comments_path)\n",
    "\n",
    "# Read data directly from S3\n",
    "submissions_df = pd.read_parquet(s3_submissions_path, storage_options={\"anon\": True})\n",
    "comments_df = pd.read_parquet(s3_comments_path, storage_options={\"anon\": True})\n",
    "\n",
    "# Display first few rows\n",
    "#print(submissions_df.head())\n",
    "#print(comments_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "037f7608-1eff-44d5-a04e-34f05675ffcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: submissions_RS_2024-01.zst_1.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_2.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_3.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_4.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_5.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_6.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_7.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_8.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_9.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_10.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_11.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_12.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_13.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_14.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_15.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_16.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_17.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_18.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_19.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_20.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_21.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_22.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_23.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_24.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_25.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_26.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_27.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_28.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_29.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_30.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_31.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_32.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_33.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_34.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_35.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_36.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_37.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_38.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_39.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_40.parquet\n",
      "Successfully loaded: submissions_RS_2024-01.zst_41.parquet\n",
      "                 author author_flair_css_class author_flair_text  created_utc  \\\n",
      "0  Puzzleheaded-Farm431                   None              None   1704655142   \n",
      "1             mfairview                   None              None   1704655144   \n",
      "2         bahamiancraig                   None              None   1704655176   \n",
      "3              Dawesite                   None              None   1704655426   \n",
      "4         No_Invite1439                   None              None   1704655607   \n",
      "\n",
      "  distinguished             domain  edited       id  is_self  locked  ...  \\\n",
      "0          None        self.AskNYC     NaN  190zx7t     True   False  ...   \n",
      "1          None        self.AskNYC     NaN  190zx8v     True   False  ...   \n",
      "2          None  self.washingtondc     NaN  190zxpz     True   False  ...   \n",
      "3          None        self.AskNYC     NaN  191017e     True   False  ...   \n",
      "4          None        self.AskNYC     NaN  1910425     True   False  ...   \n",
      "\n",
      "   retrieved_on  score                                           selftext  \\\n",
      "0    1704655160      1  Hi all, I work for a small company with very f...   \n",
      "1    1704655160      1  I live in midtown a few streets south of CP, s...   \n",
      "2    1704655197      7  Looking for bars and restaurants with American...   \n",
      "3    1704655446      1                                          [removed]   \n",
      "4    1704655625      1  Have some friends in town and we’re doing a la...   \n",
      "\n",
      "   stickied     subreddit subreddit_id  \\\n",
      "0     False        AskNYC     t5_2uqch   \n",
      "1     False        AskNYC     t5_2uqch   \n",
      "2     False  washingtondc     t5_2qi2g   \n",
      "3     False        AskNYC     t5_2uqch   \n",
      "4     False        AskNYC     t5_2uqch   \n",
      "\n",
      "                                               title  \\\n",
      "0                                  Paid family leave   \n",
      "1               How bad is store theft in your area?   \n",
      "2                                       Nitro stouts   \n",
      "3  Advice on how to locate an Asylum Seeker that ...   \n",
      "4  What’s a fun bar or dancing vibe for a Sunday ...   \n",
      "\n",
      "                                                 url  yyyy mm  \n",
      "0  https://www.reddit.com/r/AskNYC/comments/190zx...  2024  1  \n",
      "1  https://www.reddit.com/r/AskNYC/comments/190zx...  2024  1  \n",
      "2  https://www.reddit.com/r/washingtondc/comments...  2024  1  \n",
      "3  https://www.reddit.com/r/AskNYC/comments/19101...  2024  1  \n",
      "4  https://www.reddit.com/r/AskNYC/comments/19104...  2024  1  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "submissions_data = []\n",
    "\n",
    "# Loop through files 1 to 42\n",
    "for i in range(1, 42):\n",
    "    s3_submissions_path = f\"s3://{bucket_name}/{submissions[i]}\"\n",
    "    file_name = f\"submissions_RS_2024-01.zst_{i}.parquet\"\n",
    "    file_path = sub_prefix + file_name \n",
    "    try:\n",
    "        # Use pushdown predicates to filter only the desired subreddits\n",
    "        df = pd.read_parquet(s3_submissions_path, \n",
    "                             filters=[(\"subreddit\", \"in\", [\"AskNYC\", \"washingtondc\"])])\n",
    "        submissions_data.append(df)\n",
    "        print(f\"Successfully loaded: {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "submissions_df = pd.concat(submissions_data, ignore_index=True)\n",
    "print(submissions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a97482fa-67b5-4540-bcda-509ad28e2f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: comments_RC_2024-01.zst_1.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_2.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_3.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_4.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_5.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_6.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_7.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_8.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_9.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_10.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_11.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_12.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_13.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_14.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_15.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_16.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_17.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_18.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_19.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_20.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_21.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_22.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_23.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_24.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_25.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_26.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_27.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_28.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_29.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_30.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_31.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_32.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_33.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_34.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_35.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_36.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_37.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_38.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_39.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_40.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_41.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_42.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_43.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_44.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_45.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_46.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_47.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_48.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_49.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_50.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_51.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_52.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_53.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_54.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_55.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_56.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_57.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_58.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_59.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_60.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_61.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_62.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_63.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_64.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_65.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_66.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_67.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_68.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_69.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_70.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_71.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_72.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_73.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_74.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_75.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_76.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_77.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_78.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_79.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_80.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_81.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_82.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_83.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_84.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_85.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_86.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_87.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_88.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_89.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_90.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_91.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_92.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_93.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_94.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_95.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_96.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_97.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_98.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_99.parquet\n",
      "Successfully loaded: comments_RC_2024-01.zst_100.parquet\n"
     ]
    }
   ],
   "source": [
    "comments_data = []\n",
    "\n",
    "\n",
    "# Loop through files 1 to 101\n",
    "for i in range(1, 101):\n",
    "    s3_comments_path = f\"s3://{bucket_name}/{comments[i]}\"\n",
    "    file_name = f\"comments_RC_2024-01.zst_{i}.parquet\"  # Adjust the file name pattern if necessary\n",
    "    file_path = com_prefix + file_name \n",
    "    \n",
    "    try:\n",
    "        # Use pushdown predicates to filter only the desired subreddits\n",
    "        df = pd.read_parquet(s3_comments_path, \n",
    "                             filters=[(\"subreddit\", \"in\", [\"AskNYC\", \"washingtondc\"])])\n",
    "        comments_data.append(df)\n",
    "        print(f\"Successfully loaded: {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "comments_df = pd.concat(comments_data, ignore_index=True)\n",
    "comments_df[\"parent_id\"] = comments_df[\"parent_id\"].astype(str).map(lambda x: re.sub(r'^t\\d_', '', x))\n",
    "#print(comments_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77e5c8cd-2ea8-48ea-a78a-4c0ff310dde2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"author_x\":\"Puzzleheaded-Farm431\",\"author_flair_css_class_x\":null,\"author_flair_text_x\":null,\"created_utc_x\":1704655142,\"distinguished_x\":null,\"domain\":\"self.AskNYC\",\"edited_x\":null,\"id_x\":\"190zx7t\",\"is_self\":true,\"locked\":false,\"num_comments\":1,\"over_18\":false,\"quarantine\":false,\"retrieved_on_x\":1704655160,\"score_x\":1,\"selftext\":\"Hi all, I work for a small company with very few employees and no form of office really just a fabricating shop for local 28,\\nI will be applying for paid family lea\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Perform a LEFT JOIN on id (submissions) and parent_id (comments)\n",
    "merged_df = submissions_df.merge(comments_df, left_on=\"id\", right_on=\"parent_id\", how=\"left\")\n",
    "\n",
    "# Convert the merged DataFrame into a JSON array\n",
    "json_output = merged_df.to_json(orient=\"records\", lines=False)\n",
    "\n",
    "# Print the first 500 characters to check the structure\n",
    "print(json_output[:500])  \n",
    "\n",
    "# Optional: Save the JSON to a file\n",
    "with open(\"merged_data.json\", \"w\") as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec2cf85f-5670-4582-bd6d-859246da4a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['author_x', 'author_flair_css_class_x', 'author_flair_text_x',\n",
      "       'created_utc_x', 'distinguished_x', 'domain', 'edited_x', 'id_x',\n",
      "       'is_self', 'locked', 'num_comments', 'over_18', 'quarantine',\n",
      "       'retrieved_on_x', 'score_x', 'selftext', 'stickied_x', 'subreddit_x',\n",
      "       'subreddit_id_x', 'title', 'url', 'yyyy_x', 'mm_x', 'author_y',\n",
      "       'author_flair_css_class_y', 'author_flair_text_y', 'body',\n",
      "       'controversiality', 'created_utc_y', 'distinguished_y', 'edited_y',\n",
      "       'gilded', 'id_y', 'link_id', 'parent_id', 'retrieved_on_y', 'score_y',\n",
      "       'stickied_y', 'subreddit_y', 'subreddit_id_y', 'yyyy_y', 'mm_y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to reddit_AskNYC_washingtondc.json\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)\n",
    "\n",
    "# Group comments by submission id\n",
    "grouped_comments = merged_df.groupby(\"id_x\").apply(\n",
    "    lambda x: x[[\"author_y\", \"body\", \"score_y\", \"created_utc_y\", \"id_y\"]].dropna().to_dict(orient=\"records\")\n",
    ").to_dict()\n",
    "\n",
    "merged_df[\"created_utc_x\"] = pd.to_datetime(merged_df[\"created_utc_x\"], unit=\"s\")\n",
    "merged_df[\"created_utc_y\"] = pd.to_datetime(merged_df[\"created_utc_y\"], unit=\"s\")\n",
    "\n",
    "# Extract year and month\n",
    "merged_df[\"year_x\"] = merged_df[\"created_utc_x\"].dt.year\n",
    "merged_df[\"month_x\"] = merged_df[\"created_utc_x\"].dt.month\n",
    "merged_df[\"year_y\"] = merged_df[\"created_utc_y\"].dt.year\n",
    "merged_df[\"month_y\"] = merged_df[\"created_utc_y\"].dt.month\n",
    "\n",
    "final_json = {}\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    submission_id = row[\"id_x\"]\n",
    "\n",
    "    # Base submission data\n",
    "    final_json[submission_id] = {\n",
    "        \"subreddit\": row[\"subreddit_x\"],\n",
    "        \"author\": row[\"author_x\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"selftext\": row[\"selftext\"],\n",
    "        \"year\": row[\"year_x\"],  # Use extracted year\n",
    "        \"month\": row[\"month_x\"],  # Use extracted month\n",
    "        \"score\": row[\"score_x\"],\n",
    "        \"num_comments\": row[\"num_comments\"],\n",
    "        \"comments\": grouped_comments.get(submission_id, [])  # Attach comments or empty list\n",
    "    }\n",
    "\n",
    "# Save JSON to a file\n",
    "import json\n",
    "\n",
    "json_filename = \"reddit_AskNYC_washingtondc.json\"\n",
    "with open(json_filename, \"w\") as f:\n",
    "    json.dump(final_json, f, indent=4)\n",
    "\n",
    "print(f\"JSON saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4ef4e-4f39-4da7-8544-a5ccfde661f1",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
